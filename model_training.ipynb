{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('merged_testing.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['odiNumber', 'crash', 'fire', 'numberOfInjuries', 'numberOfDeaths',\n",
       "       'dateOfIncident', 'dateComplaintFiled', 'incident_filing_lag', 'vin',\n",
       "       'components', 'summary_complaint', 'products', 'Model', 'ModelYear',\n",
       "       'MODEL', 'YEAR', 'component_name', 'ODATE', 'CDATE',\n",
       "       'days_taken_in_investigation', 'subject', 'summary_investigation',\n",
       "       'similarity_score', 'recall_status'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6145 entries, 0 to 6144\n",
      "Data columns (total 24 columns):\n",
      " #   Column                       Non-Null Count  Dtype  \n",
      "---  ------                       --------------  -----  \n",
      " 0   odiNumber                    6145 non-null   int64  \n",
      " 1   crash                        6145 non-null   bool   \n",
      " 2   fire                         6145 non-null   bool   \n",
      " 3   numberOfInjuries             6145 non-null   int64  \n",
      " 4   numberOfDeaths               6145 non-null   int64  \n",
      " 5   dateOfIncident               5971 non-null   object \n",
      " 6   dateComplaintFiled           6145 non-null   object \n",
      " 7   incident_filing_lag          5971 non-null   float64\n",
      " 8   vin                          5450 non-null   object \n",
      " 9   components                   6145 non-null   object \n",
      " 10  summary_complaint            6145 non-null   object \n",
      " 11  products                     6145 non-null   object \n",
      " 12  Model                        6145 non-null   object \n",
      " 13  ModelYear                    6145 non-null   int64  \n",
      " 14  MODEL                        6145 non-null   object \n",
      " 15  YEAR                         6145 non-null   int64  \n",
      " 16  component_name               6145 non-null   object \n",
      " 17  ODATE                        6145 non-null   object \n",
      " 18  CDATE                        2914 non-null   object \n",
      " 19  days_taken_in_investigation  2914 non-null   float64\n",
      " 20  subject                      6145 non-null   object \n",
      " 21  summary_investigation        6139 non-null   object \n",
      " 22  similarity_score             6145 non-null   float64\n",
      " 23  recall_status                6145 non-null   bool   \n",
      "dtypes: bool(3), float64(3), int64(5), object(13)\n",
      "memory usage: 1.0+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data exploration and preprocessing\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Number of recalls: {df['recall_status'].sum()} out of {len(df)} ({df['recall_status'].mean()*100:.2f}%)\")\n",
    "\n",
    "# Handle dates\n",
    "date_columns = ['dateOfIncident', 'dateComplaintFiled', 'ODATE', 'CDATE', 'ReportReceivedDate']\n",
    "\n",
    "# Convert date columns to datetime format\n",
    "for col in date_columns:\n",
    "    if col in df.columns:\n",
    "        # Handle various date formats and missing values\n",
    "        df[col] = pd.to_datetime(df[col], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create features from dates where available\n",
    "if 'dateOfIncident' in df.columns and not df['dateOfIncident'].isna().all():\n",
    "    df['month_of_incident'] = df['dateOfIncident'].dt.month\n",
    "    df['year_of_incident'] = df['dateOfIncident'].dt.year\n",
    "    df['season_of_incident'] = df['dateOfIncident'].dt.month.apply(lambda x: \n",
    "                                                                 1 if pd.notna(x) and x in [12, 1, 2] else  # Winter\n",
    "                                                                 2 if pd.notna(x) and x in [3, 4, 5] else   # Spring\n",
    "                                                                 3 if pd.notna(x) and x in [6, 7, 8] else   # Summer\n",
    "                                                                 4 if pd.notna(x) and x in [9, 10, 11] else # Fall\n",
    "                                                                 np.nan)  # Handle NaN\n",
    "\n",
    "if 'dateComplaintFiled' in df.columns and not df['dateComplaintFiled'].isna().all():\n",
    "    df['month_of_complaint'] = df['dateComplaintFiled'].dt.month\n",
    "    df['year_of_complaint'] = df['dateComplaintFiled'].dt.year\n",
    "\n",
    "# Extract useful features from text columns using TF-IDF\n",
    "# First, let's clean the text data\n",
    "text_columns = ['summary_complaint', 'summary_investigation', 'subject', 'components']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clean text\n",
    "def clean_text(text):\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text)  \n",
    "    text = re.sub(r'\\s+', ' ', text)     \n",
    "    return text.strip()\n",
    "\n",
    "for col in text_columns:\n",
    "    if col in df.columns:\n",
    "        df[f'{col}_cleaned'] = df[col].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding additional features\n",
    "# Createing severity score based on injuries and deaths\n",
    "df['severity_score'] = df['numberOfInjuries'] + df['numberOfDeaths'] * 5  # Deaths weighted more heavily\n",
    "\n",
    "#calculating vehicle age at incident\n",
    "current_year = datetime.now().year\n",
    "if 'year_of_incident' in df.columns and 'ModelYear' in df.columns:\n",
    "    df['vehicle_age_at_incident'] = df['year_of_incident'] - df['ModelYear']\n",
    "    # Handle negative values (data errors)\n",
    "    df.loc[df['vehicle_age_at_incident'] < 0, 'vehicle_age_at_incident'] = np.nan\n",
    "\n",
    "# Create a feature for reliability of matching between complaint and investigation\n",
    "# Lower similarity scores might indicate incorrect grouping\n",
    "df['matching_reliability'] = df['similarity_score'].apply(\n",
    "    lambda x: 0 if x <= 0.3 else \n",
    "              1 if x <= 0.6 else \n",
    "              2 if x <= 0.8 else \n",
    "              3\n",
    ")\n",
    "\n",
    "# Create a lag feature for how quickly a complaint was filed after incident\n",
    "df['quick_complaint_filing'] = (df['incident_filing_lag'] <= 7).astype(int)  # 1 week threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define feature sets\n",
    "numerical_features = ['numberOfInjuries', 'numberOfDeaths', 'similarity_score', \n",
    "                     'severity_score', 'ModelYear', 'YEAR']\n",
    "\n",
    "# Add conditional features only if they exist\n",
    "if 'incident_filing_lag' in df.columns and not df['incident_filing_lag'].isna().all():\n",
    "    numerical_features.append('incident_filing_lag')\n",
    "if 'vehicle_age_at_incident' in df.columns and not df['vehicle_age_at_incident'].isna().all():\n",
    "    numerical_features.append('vehicle_age_at_incident')\n",
    "if 'days_taken_in_investigation' in df.columns and not df['days_taken_in_investigation'].isna().all():\n",
    "    numerical_features.append('days_taken_in_investigation')\n",
    "\n",
    "categorical_features = ['crash', 'fire', 'component_name', 'matching_reliability', 'quick_complaint_filing']\n",
    "\n",
    "# Add conditional categorical features only if they exist\n",
    "if 'Model' in df.columns:\n",
    "    categorical_features.append('Model')\n",
    "if 'month_of_incident' in df.columns and not df['month_of_incident'].isna().all():\n",
    "    categorical_features.append('month_of_incident')\n",
    "if 'season_of_incident' in df.columns and not df['season_of_incident'].isna().all():\n",
    "    categorical_features.append('season_of_incident')\n",
    "\n",
    "# Selected text features to include\n",
    "text_features = []\n",
    "for col in ['summary_complaint_cleaned', 'components_cleaned']:\n",
    "    if col in df.columns:\n",
    "        text_features.append(col)\n",
    "\n",
    "# Handle missing numerical values\n",
    "for col in numerical_features:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].fillna(df[col].median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Focus on essential columns\n",
    "essential_columns = ['recall_status']\n",
    "for col in ['summary_complaint_cleaned', 'numberOfInjuries', 'numberOfDeaths', 'crash', 'fire', 'component_name']:\n",
    "    if col in df.columns:\n",
    "        essential_columns.append(col)\n",
    "\n",
    "df_clean = df.dropna(subset=essential_columns, thresh=len(essential_columns)-1)\n",
    "\n",
    "print(f\"Shape after cleaning: {df_clean.shape}\")\n",
    "\n",
    "# Handle class imbalance\n",
    "recall_count = df_clean['recall_status'].sum()\n",
    "non_recall_count = len(df_clean) - recall_count\n",
    "print(f\"Class distribution: Recalls={recall_count}, Non-recalls={non_recall_count}\")\n",
    "\n",
    "# If severe imbalance, use class weights\n",
    "class_weights = None\n",
    "if recall_count / len(df_clean) < 0.2:  # If recalls are less than 20%\n",
    "    class_weights = {0: 1, 1: non_recall_count / recall_count}\n",
    "    print(f\"Using class weights due to imbalance: {class_weights}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature preprocessing setup\n",
    "preprocessors = []\n",
    "\n",
    "# Only add transformers for features that exist in the dataframe\n",
    "if numerical_features:\n",
    "    numerical_features = [f for f in numerical_features if f in df_clean.columns]\n",
    "    if numerical_features:\n",
    "        preprocessors.append(\n",
    "            ('num', Pipeline([\n",
    "                ('imputer', SimpleImputer(strategy='median')),\n",
    "                ('scaler', StandardScaler())\n",
    "            ]), numerical_features)\n",
    "        )\n",
    "\n",
    "if categorical_features:\n",
    "    categorical_features = [f for f in categorical_features if f in df_clean.columns]\n",
    "    if categorical_features:\n",
    "        preprocessors.append(\n",
    "            ('cat', Pipeline([\n",
    "                ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "                ('encoder', OneHotEncoder(handle_unknown='ignore'))\n",
    "            ]), categorical_features)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For text features, we need a different approach to avoid the numpy array error\n",
    "# We'll create TF-IDF features separately for each text column\n",
    "for i, text_col in enumerate(text_features):\n",
    "    if text_col in df_clean.columns:\n",
    "        # Create a custom function to extract just this column\n",
    "        def get_text_column(X, column_name=text_col):\n",
    "            if isinstance(X, pd.DataFrame):\n",
    "                return X[column_name].fillna('').values\n",
    "            return X  # If it's already processed\n",
    "\n",
    "        preprocessors.append(\n",
    "            (f'text_{i}', Pipeline([\n",
    "                ('selector', FunctionTransformer(get_text_column, validate=False)),\n",
    "                ('tfidf', TfidfVectorizer(max_features=500, ngram_range=(1, 2)))\n",
    "            ]), [text_col])  # We pass column name but it's ignored due to the custom extractor\n",
    "        )\n",
    "\n",
    "# Create the column transformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=preprocessors,\n",
    "    remainder='drop'  # This drops columns not specified in the transformers\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data\n",
    "X = df_clean.drop('recall_status', axis=1)\n",
    "y = df_clean['recall_status']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=42, stratify=y)\n",
    "# Build the model pipeline\n",
    "clf = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', RandomForestClassifier(class_weight=class_weights, \n",
    "                                         n_estimators=100, \n",
    "                                         random_state=42))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (6145, 37)\n",
      "Number of recalls: 4056 out of 6145 (66.00%)\n",
      "Shape after cleaning: (6145, 37)\n",
      "Class distribution: Recalls=4056, Non-recalls=2089\n",
      "Training the model...\n",
      "\n",
      "Model Evaluation:\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 829    7]\n",
      " [  26 1596]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.97      0.99      0.98       836\n",
      "        True       1.00      0.98      0.99      1622\n",
      "\n",
      "    accuracy                           0.99      2458\n",
      "   macro avg       0.98      0.99      0.99      2458\n",
      "weighted avg       0.99      0.99      0.99      2458\n",
      "\n",
      "\n",
      "ROC AUC Score: 0.9978\n",
      "Could not compute feature importance: Estimator selector does not provide get_feature_names_out. Did you mean to call pipeline[:-1].get_feature_names_out()?\n",
      "\n",
      "Trying different models for comparison...\n",
      "\n",
      "RandomForest Performance:\n",
      "Accuracy: 0.9866\n",
      "ROC AUC: 0.9978\n",
      "\n",
      "GradientBoosting Performance:\n",
      "Accuracy: 0.9813\n",
      "ROC AUC: 0.9940\n",
      "\n",
      "Model training and evaluation complete!\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "print(\"Training the model...\")\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = clf.predict(X_test)\n",
    "y_prob = clf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(\"\\nModel Evaluation:\")\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "print(f\"\\nROC AUC Score: {roc_auc_score(y_test, y_prob):.4f}\")\n",
    "\n",
    "# Function to get feature importance (simplified)\n",
    "def get_feature_importance(clf, feature_names=None):\n",
    "    try:\n",
    "        rf_classifier = clf.named_steps['classifier']\n",
    "        importances = rf_classifier.feature_importances_\n",
    "        \n",
    "        # If feature names is None, assign indices\n",
    "        if feature_names is None:\n",
    "            feature_names = [f\"feature_{i}\" for i in range(len(importances))]\n",
    "        \n",
    "        # Ensure feature_names and importances have same length\n",
    "        feature_names = feature_names[:len(importances)]\n",
    "        \n",
    "        # Sort feature importances\n",
    "        indices = np.argsort(importances)[::-1]\n",
    "        \n",
    "        # Limit to top features for readability\n",
    "        top_n = min(30, len(feature_names))\n",
    "        \n",
    "        # Return sorted importance info\n",
    "        return [(feature_names[i], importances[i]) for i in indices[:top_n]]\n",
    "    except Exception as e:\n",
    "        print(f\"Error getting feature importance: {e}\")\n",
    "        return []\n",
    "\n",
    "try:\n",
    "    # Get feature importance (simplified version)\n",
    "    feature_names = clf.named_steps['preprocessor'].get_feature_names_out()\n",
    "    importance = get_feature_importance(clf, feature_names)\n",
    "    \n",
    "    # Print top 10 important features\n",
    "    print(\"\\nTop 10 Important Features:\")\n",
    "    for i, (feature, importance_value) in enumerate(importance[:10]):\n",
    "        print(f\"{i+1}. {feature}: {importance_value:.4f}\")\n",
    "    \n",
    "    # Plot feature importance\n",
    "    if importance:\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        features, values = zip(*importance[:20])\n",
    "        sns.barplot(x=list(values), y=list(features))\n",
    "        plt.title('Top 20 Feature Importances for Recall Prediction')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('feature_importance.png')\n",
    "        plt.close()\n",
    "except Exception as e:\n",
    "    print(f\"Could not compute feature importance: {e}\")\n",
    "\n",
    "# Try different models to see if we can improve performance\n",
    "print(\"\\nTrying different models for comparison...\")\n",
    "\n",
    "# Define the models to try\n",
    "models = {\n",
    "    'RandomForest': RandomForestClassifier(class_weight=class_weights, n_estimators=100, random_state=42),\n",
    "    'GradientBoosting': GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
    "}\n",
    "\n",
    "# Evaluate each model\n",
    "for name, model in models.items():\n",
    "    # Create a pipeline with the model\n",
    "    model_pipe = Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('classifier', model)\n",
    "    ])\n",
    "    \n",
    "    # Train the model\n",
    "    model_pipe.fit(X_train, y_train)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    y_pred = model_pipe.predict(X_test)\n",
    "    y_prob = model_pipe.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    print(f\"\\n{name} Performance:\")\n",
    "    print(f\"Accuracy: {(y_pred == y_test).mean():.4f}\")\n",
    "    print(f\"ROC AUC: {roc_auc_score(y_test, y_prob):.4f}\")\n",
    "\n",
    "# Function to predict on new data\n",
    "def predict_recall_probability(new_data, model=clf):\n",
    "\n",
    "    # Make a copy to avoid modifying the original\n",
    "    result_df = new_data.copy()\n",
    "    \n",
    "    # Make predictions\n",
    "    try:\n",
    "        proba = model.predict_proba(new_data)[:, 1]\n",
    "        result_df['recall_probability'] = proba\n",
    "        \n",
    "        # Adding a risk category\n",
    "        result_df['risk_level'] = pd.cut(\n",
    "            result_df['recall_probability'], \n",
    "            bins=[0, 0.3, 0.7, 1.0], \n",
    "            labels=['Low', 'Medium', 'High']\n",
    "        )\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in prediction: {e}\")\n",
    "        result_df['recall_probability'] = None\n",
    "        result_df['risk_level'] = None\n",
    "        \n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
